{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "# from transformers import WavLMForSequenceClassification, WavLMProcessor\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Loading and Preparation\n",
    "## 2.1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_ravdess(data_dir):\n",
    "    emotion_labels = {\n",
    "        '01': 'neutral',\n",
    "        '02': 'calm',\n",
    "        '03': 'happy',\n",
    "        '04': 'sad',\n",
    "        '05': 'angry',\n",
    "        '06': 'fearful',\n",
    "        '07': 'disgust',\n",
    "        '08': 'surprised'\n",
    "    }\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                parts = file.split('-')\n",
    "                emotion = emotion_labels.get(parts[2])\n",
    "                file_list.append({'file_path': os.path.join(root, file), 'emotion': emotion})\n",
    "    return pd.DataFrame(file_list)\n",
    "\n",
    "def load_data_cremad(data_dir):\n",
    "    emotion_labels = {\n",
    "        'ANG': 'angry',\n",
    "        'DIS': 'disgust',\n",
    "        'FEA': 'fearful',\n",
    "        'HAP': 'happy',\n",
    "        'NEU': 'neutral',\n",
    "        'SAD': 'sad'\n",
    "    }\n",
    "    file_list = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            parts = file.split('_')\n",
    "            emotion = emotion_labels.get(parts[2])\n",
    "            if emotion:\n",
    "                file_list.append({'file_path': os.path.join(data_dir, file), 'emotion': emotion})\n",
    "    return pd.DataFrame(file_list)\n",
    "\n",
    "def load_data_tess(data_dir):\n",
    "    emotion_map = {\n",
    "        'angry': 'angry',\n",
    "        'disgust': 'disgust',\n",
    "        'fear': 'fearful',\n",
    "        'happy': 'happy',\n",
    "        'ps': 'surprised',  # Pleasant surprise\n",
    "        'sad': 'sad',\n",
    "        'neutral': 'neutral'\n",
    "    }\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                emotion = file.split('_')[2].split('.')[0]\n",
    "                emotion_label = emotion_map.get(emotion)\n",
    "                if emotion_label:\n",
    "                    file_list.append({'file_path': os.path.join(root, file), 'emotion': emotion_label})\n",
    "    return pd.DataFrame(file_list)\n",
    "\n",
    "def load_data_savee(data_dir):\n",
    "    emotion_map = {\n",
    "        'a': 'angry',\n",
    "        'd': 'disgust',\n",
    "        'f': 'fearful',\n",
    "        'h': 'happy',\n",
    "        'n': 'neutral',\n",
    "        'sa': 'sad',\n",
    "        'su': 'surprised'\n",
    "    }\n",
    "    file_list = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            emotion_code = file.split('_')[1][:2]\n",
    "            emotion_label = emotion_map.get(emotion_code)\n",
    "            if emotion_label:\n",
    "                file_list.append({'file_path': os.path.join(data_dir, file), 'emotion': emotion_label})\n",
    "    return pd.DataFrame(file_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_df = load_data_ravdess('data/RAVDESS')\n",
    "cremad_df = load_data_cremad('data/CREMA-D')\n",
    "tess_df = load_data_tess('data/TESS')\n",
    "savee_df = load_data_savee('data/SAVEE')\n",
    "\n",
    "# Combine datasets\n",
    "data_df = pd.concat([ravdess_df, cremad_df, tess_df, savee_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize emotion labels\n",
    "emotion_list = ['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised', 'calm']\n",
    "\n",
    "# Handle any missing emotions in datasets\n",
    "data_df = data_df[data_df['emotion'].isin(emotion_list)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_list)\n",
    "data_df['label'] = label_encoder.transform(data_df['emotion'])\n",
    "num_classes = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the label encoder mapping dict\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data_df\n",
    "data_df.to_csv('data_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by emotion\n",
    "emotion_counts = data_df['emotion'].value_counts().sort_index()\n",
    "emotion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(data_df, test_size=0.3, stratify=data_df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "print(f'Training samples: {len(train_df)}') \n",
    "print(f'Validation samples: {len(val_df)}')\n",
    "print(f'Test samples: {len(test_df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Baseline Models\n",
    "## 3.1: Feature Extraction for Baseline Models\n",
    "### 3.1.1: Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sr=16000):\n",
    "    y, sr = librosa.load(file_path, sr=None)  # Load with original sampling rate\n",
    "    if sr != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "    return y, sr\n",
    "\n",
    "def normalize_audio(y):\n",
    "    rms = np.sqrt(np.mean(y**2))\n",
    "    if rms > 0:\n",
    "        y_normalized = y / rms\n",
    "    else:\n",
    "        y_normalized = y\n",
    "    return y_normalized\n",
    "\n",
    "def augment_audio(y, sr):\n",
    "    augmented_data = []\n",
    "    \n",
    "    # Original\n",
    "    augmented_data.append(y)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.randn(len(y))\n",
    "    y_noise = y + 0.005 * noise\n",
    "    augmented_data.append(y_noise)\n",
    "    \n",
    "    # Time stretching\n",
    "    y_stretch = librosa.effects.time_stretch(y, rate=0.9)\n",
    "    augmented_data.append(y_stretch)\n",
    "    y_stretch = librosa.effects.time_stretch(y, rate=1.1)\n",
    "    augmented_data.append(y_stretch)\n",
    "    \n",
    "    # Pitch shifting\n",
    "    y_shift = librosa.effects.pitch_shift(y, sr=sr, n_steps=2)\n",
    "    augmented_data.append(y_shift)\n",
    "    y_shift = librosa.effects.pitch_shift(y, sr=sr, n_steps=-2)\n",
    "    augmented_data.append(y_shift)\n",
    "    \n",
    "    # Reverberation (simple simulation)\n",
    "    y_reverb = librosa.effects.preemphasis(y)\n",
    "    augmented_data.append(y_reverb)\n",
    "    \n",
    "    return augmented_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2: Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(file_path, n_mfcc=40, target_sr=16000, augment=False):\n",
    "    y, sr = load_audio(file_path, target_sr=target_sr)\n",
    "    y = normalize_audio(y)\n",
    "    \n",
    "    if augment:\n",
    "        augmented_audios = augment_audio(y, sr)\n",
    "    else:\n",
    "        augmented_audios = [y]\n",
    "    \n",
    "    features = []\n",
    "    for augmented_y in augmented_audios:\n",
    "        # Ensure consistent length by trimming or padding\n",
    "        max_length = target_sr * 3  # 3 seconds\n",
    "        # Trim or pad audio\n",
    "        if len(augmented_y) > max_length:\n",
    "            augmented_y = augmented_y[:max_length]\n",
    "        else:\n",
    "            augmented_y = np.pad(augmented_y, (0, max_length - len(augmented_y)), mode='constant')\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=augmented_y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        # Extract additional features if needed\n",
    "        chroma = librosa.feature.chroma_stft(y=augmented_y, sr=sr)\n",
    "        chroma = np.mean(chroma.T, axis=0)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=augmented_y, sr=sr)\n",
    "        spectral_contrast = np.mean(spectral_contrast.T, axis=0)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        feature_vector = np.concatenate([mfccs, chroma, spectral_contrast])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3: Extract Features for All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_df(df):\n",
    "    features = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc='Extracting features'):\n",
    "        feature = extract_features(row['file_path'], augment=False)\n",
    "        features.append(feature)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features\n",
    "X_train = extract_features_for_df(train_df)\n",
    "X_val = extract_features_for_df(val_df)\n",
    "X_test = extract_features_for_df(test_df)\n",
    "\n",
    "# Get labels\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "X_train = np.array(X_train).reshape(len(X_train), -1)\n",
    "X_val = np.array(X_val).reshape(len(X_val), -1)\n",
    "X_test = np.array(X_test).reshape(len(X_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the feature arrays\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "# Print a sample of the feature arrays\n",
    "print('Sample features from X_train:')\n",
    "print(X_train[0])\n",
    "\n",
    "print('Sample features from X_val:')\n",
    "print(X_val[0])\n",
    "\n",
    "print('Sample features from X_test:')\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data using the fitted scaler\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Train and Evaluate Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_lr = lr_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy_lr = accuracy_score(y_val, y_val_pred_lr)\n",
    "val_f1_lr = f1_score(y_val, y_val_pred_lr, average='weighted')\n",
    "\n",
    "print(f'Logistic Regression - Validation Accuracy: {val_accuracy_lr:.4f}, F1 Score: {val_f1_lr:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "val_accuracy_rf = accuracy_score(y_val, y_val_pred_rf)\n",
    "val_f1_rf = f1_score(y_val, y_val_pred_rf, average='weighted')\n",
    "\n",
    "print(f'Random Forest - Validation Accuracy: {val_accuracy_rf:.4f}, F1 Score: {val_f1_rf:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Select random samples\n",
    "num_samples = 5\n",
    "random_indices = random.sample(range(len(test_df)), num_samples)\n",
    "sample_df = test_df.iloc[random_indices]\n",
    "\n",
    "# Extract features for the selected samples\n",
    "sample_features = np.array([extract_features(row['file_path'])[0] for _, row in sample_df.iterrows()])\n",
    "sample_features = scaler.transform(sample_features)\n",
    "\n",
    "# Get true labels and predictions\n",
    "sample_labels = sample_df['label'].values\n",
    "sample_predictions_lr = lr_model.predict(sample_features)\n",
    "sample_predictions_rf = rf_model.predict(sample_features)\n",
    "\n",
    "# Display the samples\n",
    "for i, (index, row) in enumerate(sample_df.iterrows()):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Path: {row['file_path']}\")\n",
    "    print(f\"True Label: {label_encoder.inverse_transform([sample_labels[i]])[0]}\")\n",
    "    print(f\"Logistic Regression Prediction: {label_encoder.inverse_transform([sample_predictions_lr[i]])[0]}\")\n",
    "    print(f\"Random Forest Prediction: {label_encoder.inverse_transform([sample_predictions_rf[i]])[0]}\")\n",
    "    ipd.display(ipd.Audio(row['file_path']))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "test_f1_lr = f1_score(y_test, y_test_pred_lr, average='weighted')\n",
    "print(f'Logistic Regression - Test Accuracy: {test_accuracy_lr:.4f}, F1 Score: {test_f1_lr:.4f}')\n",
    "\n",
    "# Random Forest\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "test_f1_rf = f1_score(y_test, y_test_pred_rf, average='weighted')\n",
    "print(f'Random Forest - Test Accuracy: {test_accuracy_rf:.4f}, F1 Score: {test_f1_rf:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ser_wav2vec import SpeechEmotionRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val = pd.concat([train_df, val_df], ignore_index=True)\n",
    "# save the df_train_val\n",
    "df_train_val.to_csv('train_val_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size': 32,\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 30,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    'checkpoint': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = SpeechEmotionRecognition(\n",
    "    df=df_train_val,\n",
    "    model_name='facebook/wav2vec2-base',\n",
    "    batch_size=args['batch_size'],\n",
    "    lr=args['lr'],\n",
    "    num_epochs=args['epochs'],\n",
    "    checkpoint_dir=args['checkpoint_dir'],\n",
    "    gradient_accumulation_steps=args['gradient_accumulation_steps'],\n",
    "    checkpoint_path=args['checkpoint']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_best = SpeechEmotionRecognition(\n",
    "    df=df_train_val,\n",
    "    model_name='facebook/wav2vec2-base',\n",
    "    batch_size=args['batch_size'],\n",
    "    lr=args['lr'],\n",
    "    num_epochs=args['epochs'],\n",
    "    checkpoint_dir=args['checkpoint_dir'],\n",
    "    gradient_accumulation_steps=args['gradient_accumulation_steps'],\n",
    "    checkpoint_path='best_model.pt'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_best.evaluate_test_set(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "467final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
